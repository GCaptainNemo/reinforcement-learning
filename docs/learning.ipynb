{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 学习问题"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 一、介绍\n",
    "强化学习中的学习问题指智能体对环境未知(状态转移概率和奖励函数未知)，需要通过马尔科夫决策过程的采样来学习策略。学习可以分为不依赖模型(model-free)和依赖模型(model-based)两种，其中不依赖模型直接学习策略，而依赖模型则需要先对环境有一个估计(模型)，再根据这个模型来学习策略。\n",
    "\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 二、随机逼近\n",
    "对环境模型进行估计($R_s^a$和$P_{ss'}^a$)的方法本质上是一个随机逼近(stochastic approximation)问题。该问题可以表述成这样一般的形式:\n",
    "\n",
    "**随机逼近问题：** 找出方程$x = H(x)$的解，其中$x \\in \\mathbb{R}^N$，其中\n",
    "* H(x)不能计算，即$H$不可获得\n",
    "* 带噪声的独立同分布样本$H(x_i) + w_i$可以获得，且满足$i\\in[1, m], \\mathbb{E}[w] = 0$\n",
    "\n",
    "**解法**：可以通过迭代算法求解\n",
    "$$x_{t+1} = (1 - \\alpha_t)x_t + \\alpha_t[H(x_t)+ w_t] = x_t + \\alpha_t[H(x_t) + w_t - x_t]$$\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**均值估计问题**：\n",
    "\n",
    "**定理**：令$X\\in[0, 1]$是随机变量，且$x_0, x_1, ..., x_m$是$X$的i.i.d.样本观测值，定义统计量序列$\\hat{\\mu_m}$\n",
    "\n",
    "$$\\mu_0 = x_0$$\n",
    "$$\\mu_{m+1} = (1 - \\alpha_m)\\mu_m + \\alpha_m x_m$$\n",
    "\n",
    "则对$\\alpha_m \\in [0, 1], \\sum_{m\\geq 0}\\alpha_m = + \\infty, \\sum_{m \\geq 0} \\alpha_m^2 < \\infty$, 有$$\\hat{\\mu_m} \\stackrel{a.s.}\\longrightarrow \\mathbb{E}[X]$$\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 三、强化学习算法\n",
    "### 3.1 TD(0)算法\n",
    "时序差分(temporal difference)算法如下所示\n",
    "\n",
    "![TD(0)](../resources/TD0.png)\n",
    "\n",
    "### 3.2 Q-learning\n",
    "\n",
    "不同于时序差分对状态值函数迭代更新，Q-learning是对状态-行为值函数进行迭代更新。\n",
    "\n",
    "算法流程与TD(0)类似，唯一的区别在于更新对象为状态-行为值函数，且更新公式为：\n",
    "\n",
    "$$Q(s, a) \\leftarrow \\alpha Q(s, a) + (1 - \\alpha)[r(s, a) + \\gamma \\max_{a' \\in A}Q(s', a')]$$\n",
    "\n",
    "在收敛性上，Q-Learning有如下定理：\n",
    "\n",
    "**定理**：对一个马尔科夫决策过程，假设对$\\forall s \\in S, \\forall a \\in A, \\sum_{t=0}^{\\infty}\\alpha_t(s, a) = \\infty, \\sum_{t=0}^{\\infty}\\alpha_t^2(s, a) < \\infty, \\alpha_t \\in [0, 1]$, 则Q-Learning可以收敛到最优状态-行为值函数$Q^*$。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
