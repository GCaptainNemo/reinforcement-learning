{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 规划问题\n",
    "强化学习中规划(Planning)问题指环境模型已知的情况下，即$P_{ss'}^a$和$R_s^a$已知，寻找最优策略。求解规划问题算法包括策略迭代(policy iteration)、值函数迭代(value iteration)和线性规划(linear programming)。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 一、值函数迭代 \n",
    "值函数迭代算法(Value iteration algorithm)又称为VI算法，从初始状态值函数开始，迭代更新状态值函数，直到达到最优状态值函数，对应的策略也就是最优策略。\n",
    "\n",
    "![value_iteration_algorithm](../resources/VI.png)\n",
    "\n",
    "**定理**： 对任意的初始状态值函数$V_0$，由$V_{n+1} = \\Phi(V_n)$定义的迭代序列收敛到最优值函数$V^*$。\n",
    "\n",
    "**证明**: 首先构造关于值函数的度量空间，定义所有状态值函数构成的集合$X = \\{V|V \\in R^{|S|}\\}$，再在$X$上定义无穷范数$\\forall x \\in X, ||x||_\\infty = \\max_{i \\in [0, |S|]}|x_i|$，用无穷范数诱导度量$d$,则(X, d)是度量空间。又因为$X = R^{|S|}$，故(X, d)是完备度量空间。\n",
    "\n",
    "$\\ \\Phi(V)(s) = \\max_{\\pi}\\{\\sum_{a\\in A} \\pi(a|s)[R_s^a + \\gamma \\sum_{s' \\in S}P_{ss'}^a V(s')]\\}$\n",
    "\n",
    "下面证明$\\Phi$是一个**压缩映射**,$\\forall u, v \\in X$\n",
    "\n",
    "$$\\begin{aligned}\n",
    "||\\Phi(u) - \\Phi(v)||_{\\infty} &\\leq || \\gamma \\max_{\\pi}\\{\\sum_{a\\in A}\\sum_{s'\\in S}P_{ss'}^a \\pi(a|s) (u(s') - v(s'))\\}||_{\\infty} \\\\\n",
    "                     &\\leq \\gamma|| \\max_{\\pi, s'}\\{\\sum_{a\\in A}\\sum_{s'\\in S}P_{ss'}^a \\pi(a|s) (u(s') - v(s'))\\}||_\\infty \\\\\n",
    "                 &\\leq \\gamma  \\max_{\\pi, s'}\\{\\sum_{a\\in A}\\sum_{s'\\in S}P_{ss'}^a \\pi(a|s)\\} \\times ||(u(s') - v(s'))||_{\\infty} \\\\\n",
    "                 &\\leq \\gamma ||(u(s') - v(s'))||_{\\infty} \\\\\n",
    " \\end{aligned}$$\n",
    "\n",
    "因此$\\Phi$是一个**压缩映射**,由压缩映像定理(contraction mappint theorem)完备度量空间上到自身的压缩映射，存在唯一的不动点。则算子$\\Phi$存在唯一不动点$\\bar{V}$，且任意初始值$V_0$的迭代序列$\\{\\Phi^n(V_0)\\}_{n=1,2,3,...}$收敛于该不动点。\n",
    "下面说明该不动点就是最优状态值函数$V^*$，假设该不动点不是最优状态值函数，则说明该不动点对应的策略也不是最优策略$\\pi^*$，而最优策略对应的值函数会大于$\\Phi(\\bar{V})$，与$\\bar{V}$是不动点矛盾，所以不动点是最优状态值函数。\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 二、策略迭代\n",
    "\n",
    "![value_iteration_algorithm](../resources/PI.png)\n",
    "\n",
    "**定理**： 令$V_n$是策略迭代算法产生的状态值函数，则$V_n\\leq V_{n+1} \\leq V^{*}$。\n",
    "\n",
    "**证明**： 令$\\pi_{n+1}$是第$n$次迭代的策略提升，则\n",
    "\n",
    "$$R_{\\pi_{n+1}} + \\gamma P_{\\pi_{n+1}}V_n \\geq R_{\\pi_{n}} + \\gamma P_{\\pi_{n}}V_n=V_n$$\n",
    "\n",
    "即$R_{\\pi_{n+1}} \\geq (I - \\gamma P_{\\pi_{n+1}})V_n$，又因为$(I - \\gamma P_{\\pi_{n+1}})^{-1}$是保序映射：\n",
    "\n",
    "$$X \\geq 0 \\implies (I - \\gamma P_{\\pi_{n+1}})^{-1} X \\geq 0$$\n",
    "\n",
    "因此$V_{n + 1} = (I - \\gamma P_{\\pi_{n+1}})^{-1}R_{\\pi_{n +1}} \\geq V_n$\n",
    "\n",
    "注意：PI算法比VI算法收敛更快，但是PI算法每次迭代都需要解一个线性方程组，因此每次迭代的计算开销比VI大。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
